{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Setup for making models from smaller epochs of the data</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MUST SHARE KERNEL WITH NOTEBOOK `00-load-data.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TICA by epochsize setup\n",
    "- it's not really by epochsize, the total MD used in increasing\n",
    "\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get a list of epoch sizes manually created\n",
    "  - the full data will be used when plotting\n",
    "  \n",
    "- only doing this from truncated versions of longtraj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochsizes = [\n",
    "    int(step_per_ns[label_longtraj] * es)\n",
    "    for es in [\n",
    "        500,1000,2000,4000,8000,12000,16000,20000,\n",
    "        24000,28000,32000,36000,40000,44000,48000,\n",
    "        52000,56000,60000,64000,68000,72000,76000,\n",
    "        80000,84000,88000,92000,96000,100000,104000,\n",
    "        108000,112000,116000,120000,124000,128000,\n",
    "        132000,136000,140000,144000,\n",
    "    ]\n",
    "][::4]\n",
    "\n",
    "min_good_lag = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[46mGot the data primed\n"
     ]
    }
   ],
   "source": [
    "good_lag_ones = list(\n",
    "    filter(\n",
    "        lambda x: x[2][\"kwargs\"][\"lag\"] >= min_good_lag,\n",
    "        [\n",
    "            (feat, wnm, y)\n",
    "            for feat in all_models\n",
    "            for wnm in (\"ltj\", \"umi_Ca_3\", \"umi_Ca_4\", \"xma_Ca_3\")\n",
    "            for y in all_models[feat][wnm][\"tica\"]\n",
    "        ]\n",
    "        # all_models[feat][nm][\"tica\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(colorama.Back.CYAN + \"Got the data primed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HMM / downstream modeling by epochsize setup\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get a list of epoch sizes manually created for the TICA dim reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[46mParameters chosen\n"
     ]
    }
   ],
   "source": [
    "good_ones = [p for p in product(\n",
    "    list(all_models[feat]),\n",
    "    [\"kmeans\"],\n",
    "    [(\"nstates\",      n_macrostates[0])],\n",
    "    [(\"tica_lag\", l)  for l in chosen_tica_lags],\n",
    "    [(\"k\", k)         for k in n_clusters[:2]],\n",
    "    [(\"msm_lag\", l)   for l in msm_lags[::2]],\n",
    "    [(\"hmm_lag\", l)   for l in hmm_lags[:3]]\n",
    ")]\n",
    "\n",
    "print(colorama.Back.CYAN+\"Parameters chosen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - setup for the run-through in following notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "_good_ones = set(good_ones)\n",
    "\n",
    "_good_wnms = set(list(zip(*good_ones))[0])\n",
    "\n",
    "_good_tica_lags = [gtl for gtl in set(list(zip(*list(zip(*good_ones))[3]))[1]) if gtl > min_good_lag]\n",
    "\n",
    "_good_clust_pars = set(list(zip(*list(zip(*good_ones))[4]))[1])\n",
    "\n",
    "_good_hmm_lags = set(list(zip(*list(zip(*good_ones))[5]))[1])\n",
    "\n",
    "get_clusts_todo = lambda: set(zip(*itemgetter(0, 3, 4)(list(zip(*good_ones)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\infty$ DONE\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[100m\u001b[34m  DONE LOADING DATA  \n"
     ]
    }
   ],
   "source": [
    "print(colorama.Back.LIGHTBLACK_EX + colorama.Fore.BLUE + \"  DONE LOADING DATA  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "--------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
